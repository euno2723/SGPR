{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae8f48a1-95b8-4bd2-8687-2443fa4c6fce",
   "metadata": {},
   "source": [
    "# 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43ef389-6113-4b18-9797-c90893520f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring resource: rogers_girolami_data\n",
      "\n",
      "Details of data: \n",
      "Data from the textbook 'A First Course in Machine Learning'. Available from http://www.dcs.gla.ac.uk/~srogers/firstcourseml/.\n",
      "\n",
      "Please cite:\n",
      "A First Course in Machine Learning. Simon Rogers and Mark Girolami: Chapman & Hall/CRC, ISBN-13: 978-1439824146\n",
      "\n",
      "After downloading the data will take up 21949154 bytes of space.\n",
      "\n",
      "Data will be stored in /home/onoue/ods_data_cache/rogers_girolami_data.\n",
      "\n",
      "Do you wish to proceed with the download? [yes/no]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  http://www.dcs.gla.ac.uk/~srogers/firstcourseml/firstcoursemldata.tar.gz -> /home/onoue/ods_data_cache/rogers_girolami_data/firstcoursemldata.tar.gz\n",
      "|    Downloading  10.177MB     |\n",
      "|>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>|\n",
      "Extracting file.\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import cholesky, solve_triangular, LinAlgError\n",
    "from scipy.linalg import lapack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pods\n",
    "\n",
    "\n",
    "data = pods.datasets.olympic_100m_men()\n",
    "X, Y = data[\"X\"], data[\"Y\"]\n",
    "X_pred = np.linspace(X[:,0].min() - 30,\n",
    "                     X[:,0].max() + 30,\n",
    "                     500).reshape(-1,1)\n",
    "\n",
    "\n",
    "class RBF:\n",
    "    def __init__(self, variance=1., lengthscale=0.1):\n",
    "        self.variance=variance\n",
    "        self.lengthscale=lengthscale\n",
    "        # self.r = self._euclidean_distance\n",
    "        \n",
    "    def K(self, X, X2=None):\n",
    "        return self.variance * np.exp(-0.5 * (self._euc_dist(X, X2) / self.lengthscale)**2)\n",
    "        # return self._euc_dist(X, X2)\n",
    "        \n",
    "    def _euc_dist(self, X, X2):\n",
    "        if X2 is None:\n",
    "            # print(\"X2 is None\")\n",
    "            # print(X2)\n",
    "            Xsq = np.sum(np.square(X),1)\n",
    "            r2 = -2.*(np.dot(X, X.T)) + (Xsq[:,None] + Xsq[None,:]) \n",
    "            r2 = np.clip(r2, 0, np.inf)\n",
    "            np.fill_diagonal(r2, 0.)\n",
    "            return np.sqrt(r2)\n",
    "        else:\n",
    "            # print(X)\n",
    "            # print(X2)\n",
    "            X1sq = np.sum(np.square(X),1)\n",
    "            X2sq = np.sum(np.square(X2),1)\n",
    "            r2 = -2.*np.dot(X, X2.T) + (X1sq[:,None] + X2sq[None,:])\n",
    "            r2 = np.clip(r2, 0, np.inf)\n",
    "            return np.sqrt(r2)\n",
    "\n",
    "\n",
    "def generate_non_pd_mat():    \n",
    "    # Create PD matrix\n",
    "    A = np.random.randn(20, 100)\n",
    "    A = A.dot(A.T)\n",
    "    # Compute Eigdecomp\n",
    "    vals, vectors = np.linalg.eig(A)\n",
    "    # Set smallest eigenval to be negative with 5 rounds worth of jitter\n",
    "    vals[vals.argmin()] = 0\n",
    "    default_jitter = 1e-6 * np.mean(vals)\n",
    "    vals[vals.argmin()] = -default_jitter * (10 ** 3.5)\n",
    "    A_corrupt = (vectors * vals).dot(vectors.T)\n",
    "    return A_corrupt\n",
    "\n",
    "\n",
    "def custom_cholesky(A, max_tries=5):\n",
    "    A = np.ascontiguousarray(A) # パフォーマンス向上 計算結果にも影響\n",
    "    diag_A = np.diag(A)\n",
    "    jitter = diag_A.mean() * 1e-6\n",
    "    num_tries = 0\n",
    "    \n",
    "    try:\n",
    "        L = cholesky(A, lower=True)\n",
    "        return L\n",
    "    except LinAlgError:\n",
    "        num_tries += 1\n",
    "        \n",
    "    while num_tries <= max_tries and np.isfinite(jitter):\n",
    "        try:\n",
    "            L = cholesky(A + np.eye(A.shape[0]) * jitter, lower=True)\n",
    "            return L\n",
    "        except LinAlgError:\n",
    "            jitter *= 10\n",
    "            num_tries += 1\n",
    "    \n",
    "    raise LinAlgError(\"Matrix is not positive definite, even with jitter.\")\n",
    "    \n",
    "\n",
    "def symmetrify_matrix(A, upper=False):\n",
    "    triu = np.triu_indices_from(A,k=1)\n",
    "    if upper:\n",
    "        A.T[triu] = A[triu]\n",
    "    else:\n",
    "        A[triu] = A.T[triu]\n",
    "    return A\n",
    "    \n",
    "\n",
    "class GPR:\n",
    "    def __init__(self, X, y, kernel=None, mean_function=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        if kernel is None:\n",
    "            kernel = RBF()\n",
    "            self.kernel = kernel\n",
    "\n",
    "        if mean_function is None:\n",
    "            self.mean_function = np.zeros(self.X.shape[0]).reshape(-1,1)\n",
    "\n",
    "        # self.K = self.kernel.K(self.X)\n",
    "        self.mean = np.zeros(self.X.shape[0])\n",
    "\n",
    "            \n",
    "    def plot_sampled_prior(self, size=None):\n",
    "        plt.figure()\n",
    "\n",
    "        extension = np.abs(self.X.min() - self.X.max()) * 0.1\n",
    "        X = np.linspace(self.X.min() - extension,\n",
    "                        self.X.max() + extension,\n",
    "                        200).reshape(-1,1)\n",
    "        K = self.kernel.K(X)\n",
    "        L = cholesky(K)\n",
    "        n = X.shape[0]\n",
    "        samples = np.random.multivariate_normal(np.zeros(n), np.eye(n), size=size) # generate x from N(0, I)\n",
    "\n",
    "        if size == None:\n",
    "            sample = L @ samples\n",
    "            plt.plot(X.ravel(), sample, lw=1, ls='--')\n",
    "        else:\n",
    "            samples = [L @ sample for sample in samples] # y = L @ x\n",
    "            for sample in samples:\n",
    "                plt.plot(X.ravel(), sample, lw=1, ls='--')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d764f0e-9d96-48d7-8736-6635138dd132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_low_tri_mat(A):\n",
    "    A_inv, _ = lapack.dtrtri(A, lower=True)\n",
    "    return A_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1fefb-a2d8-4a3d-922f-9468c9361b22",
   "metadata": {},
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6287a974-ecc6-433f-9793-1b71fe138bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPR:\n",
    "    def __init__(self, X, y, kernel=None, noise=1.):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.noise = noise\n",
    "        \n",
    "        if kernel is None:\n",
    "            kernel = RBF()\n",
    "            self.kernel = kernel\n",
    "            \n",
    "        self._K = self.kernel(self.X)\n",
    "\n",
    "    def _inference_posterior(self):\n",
    "        m = 0 # If other mean functions are required, we need some modification here.\n",
    "        \n",
    "        Ky = self._K.copy()\n",
    "        Ky += (np.eye(Ky.shape[0]) * (self.noise+1e-8)) # unclear why 1e-8 is added. just followed the way of GPy\n",
    "        \n",
    "        LW = custom_cholesky(Ky)\n",
    "        \n",
    "        alpha = solve_triangular(LW.T, solve_triangular(LW, self.y-m, lower=True))\n",
    "        \n",
    "        return Posterior(woodbury_chol=LW, woodbury_vector=alpha, K=self._K)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f34ef-85a6-4a6e-942f-695741ae1243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posterior:\n",
    "    def __init__(self, woodbury_chol=None, woodbury_vector=None, K=None, K_chol=None):\n",
    "        self._K_chol = K_chol\n",
    "        self._K = K\n",
    "        self._woodbury_chol = woodbury_chol\n",
    "        self._woodbury_vector = woodbury_vector\n",
    "        self._woodbury_inv = woodbury_inv\n",
    "        \n",
    "        self._mean = None\n",
    "        self._covariance = None\n",
    "\n",
    "    @property\n",
    "    def woodbury_inv(self):\n",
    "        \"\"\"\n",
    "        The inverse of the woodbury matrix, in the gaussian likelihood case it is defined as\n",
    "        $$\n",
    "        (K_{xx} + \\Sigma_{xx})^{-1}\n",
    "        \\Sigma_{xx} := \\texttt{Likelihood.variance / Approximate likelihood covariance}\n",
    "        $$\n",
    "        \"\"\"\n",
    "        if self._woodbury_inv is None:\n",
    "            if self._woodbury_chol is not None:\n",
    "                self._woodbury_inv, _ = dpotri(self._woodbury_chol, lower=1)\n",
    "                self._woodbury_inv, _ = dpotrs(self.woodbury_chol, np.eye(self.woodbury_chol.shape[0]), lower=1)\n",
    "                symmetrify(self._woodbury_inv)\n",
    "        return self._woodbury_inv\n",
    "    \n",
    "    @property\n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        Posterior mean\n",
    "        $$\n",
    "        K_{xx}v\n",
    "        v := \\texttt{Woodbury vector}\n",
    "        $$\n",
    "        \"\"\"\n",
    "        if self._mean is None:\n",
    "            self._mean = self._K @ self.woodbury_vector\n",
    "        return self._mean\n",
    "\n",
    "    @property\n",
    "    def covariance(self):\n",
    "        \"\"\"\n",
    "        Posterior covariance\n",
    "        $$\n",
    "        K_{xx} - K_{xx}W_{xx}^{-1}K_{xx}\n",
    "        W_{xx} := \\texttt{Woodbury inv}\n",
    "        $$\n",
    "        \"\"\"\n",
    "        if self._covariance is None:\n",
    "            # LiK, _ = dtrtrs(self.woodbury_chol, self._K, lower=1)\n",
    "            self._covariance = (\n",
    "            np.atleast_3d(self._K) - np.tensordot(np.dot(np.atleast_3d(self.woodbury_inv).T, self._K), self._K,\n",
    "                                                  [1, 0]).T).squeeze()\n",
    "            # self._covariance = self._K - self._K.dot(self.woodbury_inv).dot(self._K)\n",
    "        return self._covariance\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def K_chol(self):\n",
    "        \"\"\"\n",
    "        Cholesky of the prior covariance K\n",
    "        \"\"\"\n",
    "        if self._K_chol is None:\n",
    "            self._K_chol = jitchol(self._K)\n",
    "        return self._K_chol\n",
    "\n",
    "    def _raw_predict(self, kern, Xnew, pred_var, full_cov=False):\n",
    "        woodbury_vector = self.woodbury_vector\n",
    "        woodbury_inv = self.woodbury_inv\n",
    "\n",
    "        Kx = kern.K(pred_var, Xnew)\n",
    "        mu = Kx.T @ woodbury_vector\n",
    "        if len(mu.shape) == 1:\n",
    "            mu = mu.reshape(-1, 1)\n",
    "        Kxx = kern.K(Xnew)\n",
    "        var = Kxx - Kx.T @ woodbury_inv @ Kx\n",
    "\n",
    "        return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be0a81-e671-4f83-a525-aaa94ff89baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd32533b-9524-403b-baa8-1ef175eee72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c101d040-ff0c-4849-98ed-77d59843f3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031719</td>\n",
       "      <td>0.879765</td>\n",
       "      <td>0.773689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672001</td>\n",
       "      <td>0.743656</td>\n",
       "      <td>0.836873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.320842</td>\n",
       "      <td>0.906843</td>\n",
       "      <td>0.181856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.031719  0.879765  0.773689\n",
       "1  0.672001  0.743656  0.836873\n",
       "2  0.320842  0.906843  0.181856"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.random.rand(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cb1f1-f71b-491e-80df-9844c308d8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070f5c6-3732-4a67-849c-61a3a492e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab7178-730f-4978-ae6b-4c49f1caa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def plot_sampled_prior(self, size=None):\n",
    "        plt.figure()\n",
    "\n",
    "        extension = np.abs(self.X.min() - self.X.max()) * 0.1\n",
    "        X = np.linspace(self.X.min() - extension,\n",
    "                        self.X.max() + extension,\n",
    "                        200).reshape(-1,1)\n",
    "        K = self.kernel.K(X)\n",
    "        L = cholesky(K)\n",
    "        n = X.shape[0]\n",
    "        samples = np.random.multivariate_normal(np.zeros(n), np.eye(n), size=size) # generate x from N(0, I)\n",
    "\n",
    "        if size == None:\n",
    "            sample = L @ samples\n",
    "            plt.plot(X.ravel(), sample, lw=1, ls='--')\n",
    "        else:\n",
    "            samples = [L @ sample for sample in samples] # y = L @ x\n",
    "            for sample in samples:\n",
    "                plt.plot(X.ravel(), sample, lw=1, ls='--')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ff225-360f-4cc2-8170-1a3babe4c230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533fde6-a58c-4702-8b10-cf66ded5c6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9a8d9-d5d0-4e17-b214-5fc7ddb8ce6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97180f0b-6eb8-4e82-b649-67b7ecd10dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4182a-a7cb-4945-9d40-84062ff4c8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e7e35-72ea-4911-ba5e-bab0c7e5ece0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a7a9e-e5e4-4e27-97d6-1419ffb15bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7f75f-a1f5-427d-9d10-d3757b87cae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcf666-5a23-46c6-a85c-5a375ba8a560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789b4b1-9024-40c1-8f10-c211d3291085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538837a-a838-4f36-ae31-9a48dc61e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FITC(LatentFunctionInference):\n",
    "    \"\"\"\n",
    "    An object for inference when the likelihood is Gaussian, but we want to do sparse inference.\n",
    "\n",
    "    The function self.inference returns a Posterior object, which summarizes\n",
    "    the posterior.\n",
    "\n",
    "    \"\"\"\n",
    "    const_jitter = 1e-6\n",
    "\n",
    "    def inference(self, kern, X, Z, likelihood, Y, mean_function=None, Y_metadata=None):\n",
    "        assert mean_function is None, \"inference with a mean function not implemented\"\n",
    "\n",
    "        num_inducing, _ = Z.shape\n",
    "        num_data, output_dim = Y.shape\n",
    "\n",
    "        #make sure the noise is not hetero\n",
    "        sigma_n = likelihood.gaussian_variance(Y_metadata)\n",
    "        if sigma_n.size >1:\n",
    "            raise NotImplementedError(\"no hetero noise with this implementation of FITC\")\n",
    "\n",
    "        Kmm = kern.K(Z)\n",
    "        Knn = kern.Kdiag(X)\n",
    "        Knm = kern.K(X, Z)\n",
    "        U = Knm\n",
    "\n",
    "        #factor Kmm\n",
    "        diag.add(Kmm, self.const_jitter)\n",
    "        Kmmi, L, Li, _ = pdinv(Kmm)\n",
    "\n",
    "        #compute beta_star, the effective noise precision\n",
    "        LiUT = np.dot(Li, U.T)\n",
    "        sigma_star = Knn + sigma_n - np.sum(np.square(LiUT),0)\n",
    "        beta_star = 1./sigma_star\n",
    "\n",
    "        # Compute and factor A\n",
    "        A = tdot(LiUT*np.sqrt(beta_star)) + np.eye(num_inducing)\n",
    "        LA = jitchol(A)\n",
    "\n",
    "        # back substutue to get b, P, v\n",
    "        URiy = np.dot(U.T*beta_star,Y)\n",
    "        tmp, _ = dtrtrs(L, URiy, lower=1)\n",
    "        b, _ = dtrtrs(LA, tmp, lower=1)\n",
    "        tmp, _ = dtrtrs(LA, b, lower=1, trans=1)\n",
    "        v, _ = dtrtrs(L, tmp, lower=1, trans=1)\n",
    "        tmp, _ = dtrtrs(LA, Li, lower=1, trans=0)\n",
    "        P = tdot(tmp.T)\n",
    "\n",
    "        #compute log marginal\n",
    "        log_marginal = -0.5*num_data*output_dim*np.log(2*np.pi) + \\\n",
    "                       -np.sum(np.log(np.diag(LA)))*output_dim + \\\n",
    "                       0.5*output_dim*np.sum(np.log(beta_star)) + \\\n",
    "                       -0.5*np.sum(np.square(Y.T*np.sqrt(beta_star))) + \\\n",
    "                       0.5*np.sum(np.square(b))\n",
    "        #compute dL_dR\n",
    "        Uv = np.dot(U, v)\n",
    "        dL_dR = 0.5*(np.sum(U*np.dot(U,P), 1) - 1./beta_star + np.sum(np.square(Y), 1) - 2.*np.sum(Uv*Y, 1) + np.sum(np.square(Uv), 1))*beta_star**2\n",
    "\n",
    "\n",
    "        # Compute dL_dKmm\n",
    "        vvT_P = tdot(v.reshape(-1,1)) + P\n",
    "        dL_dK = 0.5*(Kmmi - vvT_P)\n",
    "        KiU = np.dot(Kmmi, U.T)\n",
    "        dL_dK += np.dot(KiU*dL_dR, KiU.T)\n",
    "\n",
    "        # Compute dL_dU\n",
    "        vY = np.dot(v.reshape(-1,1),Y.T)\n",
    "        dL_dU = vY - np.dot(vvT_P, U.T)\n",
    "        dL_dU *= beta_star\n",
    "        dL_dU -= 2.*KiU*dL_dR\n",
    "\n",
    "        dL_dthetaL = likelihood.exact_inference_gradients(dL_dR)\n",
    "        grad_dict = {'dL_dKmm': dL_dK, 'dL_dKdiag':dL_dR, 'dL_dKnm':dL_dU.T, 'dL_dthetaL':dL_dthetaL}\n",
    "\n",
    "        #construct a posterior object\n",
    "        post = Posterior(woodbury_inv=Kmmi-P, woodbury_vector=v, K=Kmm, mean=None, cov=None, K_chol=L)\n",
    "\n",
    "        return post, log_marginal, grad_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b5844-1342-4d09-a95f-f888bf118e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e18eb-c44c-42a5-b7a8-a036045aadd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d5a17-9665-4e2f-8592-09db9c0d948b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a88f0c-aa2e-42f7-a3b3-1462c0a0bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6cf0f366-4b09-46cf-ad77-bd310d411fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".pd{\n",
       "    font-family: \"Courier New\", Courier, monospace !important;\n",
       "    width: 100%;\n",
       "    padding: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<p class=pd>\n",
       "<b>Model</b>: GP regression<br>\n",
       "<b>Objective</b>: 756.8969178418422<br>\n",
       "<b>Number of Parameters</b>: 3<br>\n",
       "<b>Number of Optimization Parameters</b>: 3<br>\n",
       "<b>Updates</b>: True<br>\n",
       "</p>\n",
       "<style type=\"text/css\">\n",
       ".tg  {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\"><tr><th><b>  GP_regression.         </b></th><th><b>value</b></th><th><b>constraints</b></th><th><b>priors</b></th></tr>\n",
       "<tr><td class=tg-left>  rbf.variance           </td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  rbf.lengthscale        </td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  Gaussian_noise.variance</td><td class=tg-right>  1.0</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<GPy.models.gp_regression.GPRegression at 0x7fa8b6d9ddc0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GPy\n",
    "m = GPy.models.GPRegression(X, Y)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28db35-d94a-43d6-98f9-5d5bc8c11aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
